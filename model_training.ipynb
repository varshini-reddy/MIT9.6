{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "tgmfcn8bp1Td",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1a2c3f4-2b00-4dfe-f7b3-63612f15d5a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data exists\n"
          ]
        }
      ],
      "source": [
        "import os.path\n",
        "\n",
        "if os.path.exists(\"/content/Image_Data\"):\n",
        "  print(\"Data exists\")\n",
        "else:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "  !unzip \"/content/drive/MyDrive/MIT6.819 Project/Image_Data.zip\" -d \"/content/\"\n",
        "  # !mv \"/content/drive/My Drive/MIT6.819 Project/utils.py\" \"/content/\"\n",
        "  !mv \"/content/drive/My Drive/MIT6.819 Project/model_weights.h5\" \"/content/\"\n"
      ],
      "id": "tgmfcn8bp1Td"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "0a0e835f"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import re\n",
        "import os.path\n",
        "import cv2\n",
        "       \n",
        "import sys\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import backend as K\n",
        "try:\n",
        "  import tensorflow_addons as tfa\n",
        "except:\n",
        "  !pip install tensorflow_addons\n",
        "  import tensorflow_addons as tfa\n",
        "  \n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow.keras.layers import Conv2D, BatchNormalization, MaxPool2D, Dense, Flatten, Dropout\n",
        "from tensorflow.keras.layers import Input, Concatenate, InputLayer, Conv2DTranspose, Resizing\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import warnings \n",
        "warnings.filterwarnings('always')\n",
        "from skimage.transform import warp_polar, warp_coords\n",
        "try:\n",
        "  from tensorflow_graphics.math.math_helpers import cartesian_to_spherical_coordinates, spherical_to_cartesian_coordinates\n",
        "except:\n",
        "  !pip install --upgrade tensorflow-graphics\n",
        "  from tensorflow_graphics.math.math_helpers import cartesian_to_spherical_coordinates, spherical_to_cartesian_coordinates\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "\n",
        "# Helper code files\n",
        "import time\n",
        "sys.path.append('../')\n",
        "from utils import get_dataset, set_model_weights, lp_architecture\n",
        "from IPython.display import clear_output\n"
      ],
      "id": "0a0e835f"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "a3aa622b"
      },
      "outputs": [],
      "source": [
        "# Global Variables\n",
        "input_shape = (224, 224, 3)\n",
        "num_classes = 1000\n",
        "batch_size = 1\n"
      ],
      "id": "a3aa622b"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "collapsed": true,
        "id": "f354f3e5-9021-419d-8fc1-17081bb0bddd"
      },
      "outputs": [],
      "source": [
        "input_layer, output_layer = lp_architecture()\n",
        "model = Model(inputs=input_layer, outputs=output_layer, name=\"LP_MODEL\")\n"
      ],
      "id": "f354f3e5-9021-419d-8fc1-17081bb0bddd"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "e28ca46b-0b08-4c1f-adc6-ef9de7e2a787"
      },
      "outputs": [],
      "source": [
        "model = set_model_weights(model, model_type=\"custom\", \n",
        "                          path=\"/content/model_weights.h5\")\n"
      ],
      "id": "e28ca46b-0b08-4c1f-adc6-ef9de7e2a787"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90e8553d-be05-494d-8056-6452d5ce310c",
        "outputId": "ff1262f4-ee51-473a-cb53-4aca3e3ff6e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "EPOCH: 7\n",
            "\tBATCH NUMBER: 0\n",
            "\tBATCH NUMBER: 50\n",
            "\tBATCH NUMBER: 100\n",
            "\t\tUPDATED MODEL WEIGHTS\n",
            "\n",
            "EPOCH: 8\n",
            "\tBATCH NUMBER: 0\n",
            "\tBATCH NUMBER: 50\n",
            "\tBATCH NUMBER: 100\n",
            "\t\tUPDATED MODEL WEIGHTS\n",
            "\n",
            "EPOCH: 9\n",
            "\tBATCH NUMBER: 0\n",
            "\tBATCH NUMBER: 50\n"
          ]
        }
      ],
      "source": [
        "# GRADIENT TAPE\n",
        "\n",
        "# Algorithm parameters\n",
        "loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
        "metrics = tf.keras.metrics.Accuracy()\n",
        "num_epochs = 30\n",
        "\n",
        "path=\"/content/Image_Data/\"\n",
        "\n",
        "train_data = get_dataset(batch_size, False, path=path)\n",
        "\n",
        "# Since batch_size is 1, each epoch will see steps_per_epoch number of images\n",
        "steps_per_epoch = 128\n",
        "\n",
        "for i in range(num_epochs):\n",
        "    print(\"\\nEPOCH:\", i+1)\n",
        "    loss = 0\n",
        "    \n",
        "    for bs in range(steps_per_epoch):\n",
        "        if bs%50==0:\n",
        "            print(\"\\tBATCH NUMBER:\",bs)\n",
        "        try:\n",
        "            t1 = train_data.next()\n",
        "        except:\n",
        "            print(\"\\t\\tFresh Images\")\n",
        "            train_data = get_dataset(batch_size, False, path=path)\n",
        "            t1 = train_data.next()\n",
        "        \n",
        "        with tf.GradientTape() as tape:\n",
        "            output = model(t1[0])\n",
        "            loss+= loss_fn(t1[1], output)\n",
        "            \n",
        "    gradients = tape.gradient(loss, model.trainable_weights)\n",
        "    \n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_weights))    \n",
        "    model.save_weights(\"model_weights.h5\")\n",
        "    print(\"\\t\\tUPDATED MODEL WEIGHTS\")\n",
        "\n",
        "    if i%5==0:\n",
        "      # files.download(\"model_weights.h5\")\n",
        "      clear_output(wait=True)\n",
        "\n"
      ],
      "id": "90e8553d-be05-494d-8056-6452d5ce310c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zmEWO3lLsVvS"
      },
      "outputs": [],
      "source": [
        "model.save_weights(\"model_weights_final.h5\")\n",
        "files.download(\"model_weights_final.h5\")\n"
      ],
      "id": "zmEWO3lLsVvS"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3uGJsZ70VEd3"
      },
      "outputs": [],
      "source": [
        ""
      ],
      "id": "3uGJsZ70VEd3"
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "model_training_updated.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}